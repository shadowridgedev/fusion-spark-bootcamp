{
  "id": "sessionize",
  "type": "script",
  "script": "val readFromSolrOpts = Map(\"collection\" -> \"apachelogs\",\n  \"query\" -> \"+clientip:[* TO *] +ts:[* TO *] +bytes:[* TO *] +verb:[* TO *] +response:[* TO *]\",\n  \"split_field\" -> \"_version_\",\n  \"splits_per_shard\" -> \"4\",\n  \"fields\" -> \"id,_version_,clientip,ts,bytes,response,verb\")\n\n\nvar logEvents = sqlContext.read.format(\"solr\").options(readFromSolrOpts).load\nlogEvents.cache()\nlogEvents.registerTempTable(\"logs\")\n\nsqlContext.udf.register(\"ts2ms\", (d: java.sql.Timestamp) => d.getTime)\nsqlContext.udf.register(\"asInt\", (b: String) => b.toInt)\n\nval sessions = sqlContext.sql(\n  \"\"\"\n    |SELECT *, sum(IF(diff_ms > 30000, 1, 0))\n    |OVER (PARTITION BY clientip ORDER BY ts) session_id\n    |FROM (SELECT *, ts2ms(ts) - lag(ts2ms(ts))\n    |OVER (PARTITION BY clientip ORDER BY ts) as diff_ms FROM logs) tmp\n  \"\"\".stripMargin)\nsessions.registerTempTable(\"sessions\")\nsessions.cache()\n\nvar sessionsAgg = sqlContext.sql(\n  \"\"\"\n        |SELECT concat_ws('||', clientip,session_id) as id,\n        |       first(clientip) as clientip,\n        |       min(ts) as session_start,\n        |       max(ts) as session_end,\n        |       (ts2ms(max(ts)) - ts2ms(min(ts))) as session_len_ms_l,\n        |       sum(asInt(bytes)) as total_bytes_l,\n        |       count(*) as total_requests_l\n        |FROM sessions\n        |GROUP BY clientip,session_id\n  \"\"\".stripMargin)\n\nval writeToSolrOpts = Map(\"collection\" -> \"apachelogs_signals_aggr\")\nsessionsAgg.write.format(\"solr\").options(writeToSolrOpts).mode(org.apache.spark.sql.SaveMode.Overwrite).save"
}
